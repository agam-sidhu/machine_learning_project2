{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using a Neural Network to fit the California Housing data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1600899846874,
     "user": {
      "displayName": "Hao Ji",
      "photoUrl": "",
      "userId": "12290693972539811867"
     },
     "user_tz": 420
    },
    "id": "Q71FjRQDhMdI",
    "outputId": "ceae955b-2e3c-44ca-f5a2-967b026f0d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 13)\n"
     ]
    }
   ],
   "source": [
    "# California Housing dataset\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# load data from csv file\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\", \"housing.csv\")\n",
    "housing = pd.read_csv('housing.csv')\n",
    "\n",
    "# Using the setting inplace=False, drop() creates a copy of the data and does not affect housing dataset\n",
    "housing_data = housing.drop(\"median_house_value\", axis=1, inplace=False)\n",
    "housing_target = housing[\"median_house_value\"].copy()\n",
    "feature_names = list(housing_data.columns)\n",
    "\n",
    "#  Transformation pipeline at https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, feature_names[:-1]),\n",
    "    ('cat', OneHotEncoder(), [feature_names[-1]]),\n",
    "])\n",
    "\n",
    "housing_preprocessed = full_pipeline.fit_transform(housing_data)\n",
    "print(housing_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_preprocessed\n",
    "y = housing_target.to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (for comparsion) Using scikit-learn's Linear Regression model to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1600899872762,
     "user": {
      "displayName": "Hao Ji",
      "photoUrl": "",
      "userId": "12290693972539811867"
     },
     "user_tz": 420
    },
    "id": "kpzf_PWRtXz3",
    "outputId": "c5871fea-80b2-444d-cf85-bf91a87b072b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training error : 0.640\n",
      "model testing error: 0.666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# documentation at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"model training error : %.3f\" % lr.score(X_train, y_train))\n",
    "print(\"model testing error: %.3f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1867,
     "status": "ok",
     "timestamp": 1600899847862,
     "user": {
      "displayName": "Hao Ji",
      "photoUrl": "",
      "userId": "12290693972539811867"
     },
     "user_tz": 420
    },
    "id": "v5ScwbRUuT8V"
   },
   "source": [
    "# Using PyTorch nn.Sequential() to build a neural network to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 56069459968.0000, Validation loss 56157859840.0000\n",
      "Epoch 100, Training loss 16399368192.0000, Validation loss 16369868800.0000\n",
      "Epoch 200, Training loss 10471561216.0000, Validation loss 10440920064.0000\n",
      "Epoch 300, Training loss 8339233280.0000, Validation loss 8283198464.0000\n",
      "Epoch 400, Training loss 7417796096.0000, Validation loss 7304110592.0000\n",
      "Epoch 500, Training loss 6658002432.0000, Validation loss 6543735808.0000\n",
      "Epoch 600, Training loss 6082086912.0000, Validation loss 6003782144.0000\n",
      "Epoch 700, Training loss 5741580288.0000, Validation loss 5602222592.0000\n",
      "Epoch 800, Training loss 5474632704.0000, Validation loss 5359897600.0000\n",
      "Epoch 900, Training loss 5291606016.0000, Validation loss 5117635072.0000\n",
      "Epoch 1000, Training loss 5137945600.0000, Validation loss 4947048448.0000\n",
      "Epoch 1100, Training loss 4956376064.0000, Validation loss 4799183360.0000\n",
      "Epoch 1200, Training loss 4848791552.0000, Validation loss 4620240896.0000\n",
      "Epoch 1300, Training loss 4743720960.0000, Validation loss 4520429568.0000\n",
      "Epoch 1400, Training loss 4573657088.0000, Validation loss 4331883008.0000\n",
      "Epoch 1500, Training loss 4456257536.0000, Validation loss 4276166144.0000\n",
      "Epoch 1600, Training loss 4361408512.0000, Validation loss 4153128448.0000\n",
      "Epoch 1700, Training loss 4276520704.0000, Validation loss 4072081152.0000\n",
      "Epoch 1800, Training loss 4207219456.0000, Validation loss 4017344000.0000\n",
      "Epoch 1900, Training loss 4144567296.0000, Validation loss 3985489408.0000\n",
      "Epoch 2000, Training loss 4144743168.0000, Validation loss 3949306624.0000\n",
      "Epoch 2100, Training loss 4056809728.0000, Validation loss 3936777984.0000\n",
      "Epoch 2200, Training loss 4059682816.0000, Validation loss 3863202560.0000\n",
      "Epoch 2300, Training loss 4073184768.0000, Validation loss 3839818752.0000\n",
      "Epoch 2400, Training loss 3987672064.0000, Validation loss 3797380352.0000\n",
      "Epoch 2500, Training loss 3925658112.0000, Validation loss 3768120832.0000\n",
      "Epoch 2600, Training loss 3930889216.0000, Validation loss 3733262080.0000\n",
      "Epoch 2700, Training loss 3887667200.0000, Validation loss 3711139072.0000\n",
      "Epoch 2800, Training loss 3853482752.0000, Validation loss 3708981248.0000\n",
      "Epoch 2900, Training loss 3845746944.0000, Validation loss 3598322688.0000\n",
      "Epoch 3000, Training loss 3821241088.0000, Validation loss 3650540032.0000\n",
      "Epoch 3100, Training loss 3822970880.0000, Validation loss 3683364096.0000\n",
      "Epoch 3200, Training loss 3778653440.0000, Validation loss 3694682368.0000\n",
      "Epoch 3300, Training loss 3757417728.0000, Validation loss 3624374016.0000\n",
      "Epoch 3400, Training loss 3758451712.0000, Validation loss 3595535360.0000\n",
      "Epoch 3500, Training loss 3754306560.0000, Validation loss 3542450688.0000\n",
      "Epoch 3600, Training loss 3745813760.0000, Validation loss 3574651392.0000\n",
      "Epoch 3700, Training loss 3731200256.0000, Validation loss 3556792576.0000\n",
      "Epoch 3800, Training loss 3770369280.0000, Validation loss 3650612224.0000\n",
      "Epoch 3900, Training loss 3753538560.0000, Validation loss 3597406208.0000\n",
      "Epoch 4000, Training loss 3680558592.0000, Validation loss 3564341760.0000\n",
      "Epoch 4100, Training loss 3643457280.0000, Validation loss 3496351744.0000\n",
      "Epoch 4200, Training loss 3666710016.0000, Validation loss 3566300160.0000\n",
      "Epoch 4300, Training loss 3631995392.0000, Validation loss 3546954752.0000\n",
      "Epoch 4400, Training loss 3652062720.0000, Validation loss 3527160320.0000\n",
      "Epoch 4500, Training loss 3652929792.0000, Validation loss 3504442880.0000\n",
      "Epoch 4600, Training loss 3604484096.0000, Validation loss 3473333760.0000\n",
      "Epoch 4700, Training loss 3669179904.0000, Validation loss 3438847488.0000\n",
      "Epoch 4800, Training loss 3574436608.0000, Validation loss 3489771776.0000\n",
      "Epoch 4900, Training loss 3591668992.0000, Validation loss 3437658880.0000\n",
      "Epoch 5000, Training loss 3573544704.0000, Validation loss 3438286336.0000\n",
      "Test loss: 3438286336.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "#converts x_train, x_test, y_train, y_test into tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).squeeze() \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).squeeze()\n",
    "\n",
    "#creating Sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64,1),\n",
    ")\n",
    "\n",
    "#Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "#set max number of iterations\n",
    "n_epochs = 5000\n",
    "#optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Set training mode\n",
    "    model.train() \n",
    "    y_pred_train = model(X_train_tensor)\n",
    "    loss_train = loss_fn(y_pred_train.squeeze(), y_train_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    loss_train.backward()\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #prints the training and validation loss\n",
    "    if epoch == 1 or epoch % 100 == 0:\n",
    "        # Set evaluation mode\n",
    "        model.eval()\n",
    "        y_pred_val = model(X_test_tensor)\n",
    "        loss_val = loss_fn(y_pred_val.squeeze(), y_test_tensor)  \n",
    "\n",
    "        print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f}, Validation loss {loss_val.item():.4f}\")\n",
    "\n",
    "# Set evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#calculates test loss  \n",
    "y_pred_test = model(X_test_tensor)\n",
    "loss_test = loss_fn(y_pred_test.squeeze(), y_test_tensor) \n",
    "\n",
    "print(f\"Test loss: {loss_test.item():.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subclassing nn.Module to build a neural network to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 56069574656.0000, Validation loss 56181940224.0000\n",
      "Epoch 100, Training loss 16129658880.0000, Validation loss 16072100864.0000\n",
      "Epoch 200, Training loss 9838605312.0000, Validation loss 9807795200.0000\n",
      "Epoch 300, Training loss 7613194240.0000, Validation loss 7558524928.0000\n",
      "Epoch 400, Training loss 6527055360.0000, Validation loss 6406428672.0000\n",
      "Epoch 500, Training loss 5899644928.0000, Validation loss 5764470272.0000\n",
      "Epoch 600, Training loss 5520250368.0000, Validation loss 5344396800.0000\n",
      "Epoch 700, Training loss 5207925760.0000, Validation loss 5069363200.0000\n",
      "Epoch 800, Training loss 4962328064.0000, Validation loss 4839010816.0000\n",
      "Epoch 900, Training loss 4825259008.0000, Validation loss 4614091264.0000\n",
      "Epoch 1000, Training loss 4650229248.0000, Validation loss 4448231936.0000\n",
      "Epoch 1100, Training loss 4515095040.0000, Validation loss 4375314432.0000\n",
      "Epoch 1200, Training loss 4408342528.0000, Validation loss 4182234112.0000\n",
      "Epoch 1300, Training loss 4328048640.0000, Validation loss 4173166848.0000\n",
      "Epoch 1400, Training loss 4296786944.0000, Validation loss 4044544512.0000\n",
      "Epoch 1500, Training loss 4143027456.0000, Validation loss 3945312512.0000\n",
      "Epoch 1600, Training loss 4068769792.0000, Validation loss 3864460800.0000\n",
      "Epoch 1700, Training loss 4029243648.0000, Validation loss 3800153088.0000\n",
      "Epoch 1800, Training loss 3962646528.0000, Validation loss 3811799296.0000\n",
      "Epoch 1900, Training loss 3949224704.0000, Validation loss 3736790784.0000\n",
      "Epoch 2000, Training loss 3842555392.0000, Validation loss 3727091200.0000\n",
      "Epoch 2100, Training loss 3846119424.0000, Validation loss 3679138304.0000\n",
      "Epoch 2200, Training loss 3822941184.0000, Validation loss 3667802368.0000\n",
      "Epoch 2300, Training loss 3806633216.0000, Validation loss 3633798144.0000\n",
      "Epoch 2400, Training loss 3738263808.0000, Validation loss 3634773504.0000\n",
      "Epoch 2500, Training loss 3751604736.0000, Validation loss 3529403136.0000\n",
      "Epoch 2600, Training loss 3696272896.0000, Validation loss 3512906496.0000\n",
      "Epoch 2700, Training loss 3652230144.0000, Validation loss 3519135232.0000\n",
      "Epoch 2800, Training loss 3708907264.0000, Validation loss 3577358336.0000\n",
      "Epoch 2900, Training loss 3661261056.0000, Validation loss 3520651776.0000\n",
      "Epoch 3000, Training loss 3677624320.0000, Validation loss 3494442496.0000\n",
      "Epoch 3100, Training loss 3647055616.0000, Validation loss 3465222912.0000\n",
      "Epoch 3200, Training loss 3576921344.0000, Validation loss 3395206400.0000\n",
      "Epoch 3300, Training loss 3563677952.0000, Validation loss 3353290240.0000\n",
      "Epoch 3400, Training loss 3572692480.0000, Validation loss 3404413952.0000\n",
      "Epoch 3500, Training loss 3528187904.0000, Validation loss 3337202688.0000\n",
      "Epoch 3600, Training loss 3621086720.0000, Validation loss 3397069824.0000\n",
      "Epoch 3700, Training loss 3499532544.0000, Validation loss 3365733632.0000\n",
      "Epoch 3800, Training loss 3528931072.0000, Validation loss 3334622976.0000\n",
      "Epoch 3900, Training loss 3435961088.0000, Validation loss 3289031424.0000\n",
      "Epoch 4000, Training loss 3485146368.0000, Validation loss 3320325120.0000\n",
      "Epoch 4100, Training loss 3498853632.0000, Validation loss 3298597376.0000\n",
      "Epoch 4200, Training loss 3457555968.0000, Validation loss 3274221824.0000\n",
      "Epoch 4300, Training loss 3446313216.0000, Validation loss 3259612672.0000\n",
      "Epoch 4400, Training loss 3471724032.0000, Validation loss 3321873408.0000\n",
      "Epoch 4500, Training loss 3415514880.0000, Validation loss 3296402688.0000\n",
      "Epoch 4600, Training loss 3398407168.0000, Validation loss 3315962368.0000\n",
      "Epoch 4700, Training loss 3503158272.0000, Validation loss 3284076800.0000\n",
      "Epoch 4800, Training loss 3429276672.0000, Validation loss 3277004032.0000\n",
      "Epoch 4900, Training loss 3446453504.0000, Validation loss 3279881472.0000\n",
      "Epoch 5000, Training loss 3457763584.0000, Validation loss 3336744704.0000\n",
      "Test loss: 3336744704.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class SubclassNetwork(nn.Module):\n",
    "    def __init__(self, set_size):\n",
    "        super().__init__()  # <1>\n",
    "        \n",
    "        self.hidden_linear = nn.Linear(set_size, 64)\n",
    "        self.hidden_activation = nn.Tanh()\n",
    "        self.output_linear = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_t = self.hidden_linear(input)\n",
    "        activated_t = self.hidden_activation(hidden_t)\n",
    "        output_t = self.output_linear(activated_t)\n",
    "        \n",
    "        return output_t\n",
    "    \n",
    "set_size = X_train.shape[1]  \n",
    "model = SubclassNetwork(set_size)\n",
    "\n",
    "#training loop function\n",
    "def trainingLoop(model, X_train, X_test, y_train, y_test, opti, n_epochs):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "    # Set training mode\n",
    "        model.train() \n",
    "        y_pred_train = model(X_train)\n",
    "        loss_train = loss_fn(y_pred_train.squeeze(), y_train)  # Squeeze the predictions\n",
    "    # Backward pass\n",
    "        loss_train.backward()\n",
    "    # Update weights\n",
    "        opti.step()\n",
    "    # Zero gradients\n",
    "        opti.zero_grad()\n",
    "\n",
    "        #prints training and validation loss\n",
    "        if epoch == 1 or epoch % 100 == 0:\n",
    "        # Set evaluation mode\n",
    "            model.eval()\n",
    "            y_pred_val = model(X_test)\n",
    "            loss_val = loss_fn(y_pred_val.squeeze(), y_test)  # Squeeze the predictions\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f}, Validation loss {loss_val.item():.4f}\")\n",
    "\n",
    "#loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "#optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "#converts x_train, x_test, y_train, y_test into tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).squeeze() \n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).squeeze()\n",
    "trainingLoop(model, X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, optimizer, 5000)\n",
    "\n",
    "#set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "#calculates test loss  \n",
    "y_pred_test = model(X_test_tensor)\n",
    "loss_test = loss_fn(y_pred_test.squeeze(), y_test_tensor)  # Squeeze the predictions\n",
    "\n",
    "print(f\"Test loss: {loss_test.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObgbZ308XwxAT/0fzUyBBf",
   "collapsed_sections": [],
   "name": "7 - Transformation PIpelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
